{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d42a0d1",
   "metadata": {},
   "source": [
    "# Évaluation du modèle DQN entraîné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1427ae5e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_agent(env, agent, n_episodes=50):\n",
    "    \"\"\"\n",
    "    Évalue les performances de l'agent sur n_episodes sans exploration.\n",
    "    \"\"\"\n",
    "    print(f\"--- Démarrage de l'évaluation sur {n_episodes} épisodes ---\")\n",
    "    \n",
    "    # 1. Sauvegarder l'epsilon actuel et le mettre à 0 pour le test (Exploitation pure)\n",
    "    original_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0.0\n",
    "    \n",
    "    # Mettre le réseau en mode évaluation (pas de calcul de gradients inutile)\n",
    "    agent.q_network.eval()\n",
    "    \n",
    "    game_scores = []      # Vrais scores du jeu (points affichés à l'écran)\n",
    "    total_rewards = []    # Rewards cumulées (ce que l'IA optimise)\n",
    "    steps_survived = []   # Durée de survie\n",
    "\n",
    "    for i in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "        \n",
    "        while not done:\n",
    "            # L'agent choisit l'action (sans aléatoire car epsilon=0)\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            episode_steps += 1\n",
    "            \n",
    "        # Fin de l'épisode\n",
    "        game_scores.append(env.game.score) # On récupère le score interne du jeu\n",
    "        total_rewards.append(episode_reward)\n",
    "        steps_survived.append(episode_steps)\n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"Episode {i+1}/{n_episodes} - Score Jeu: {env.game.score} - Reward: {episode_reward:.2f}\")\n",
    "\n",
    "    # 2. Restaurer la configuration de l'agent\n",
    "    agent.epsilon = original_epsilon\n",
    "    agent.q_network.train() # Remettre en mode train si besoin\n",
    "    \n",
    "    # 3. Calcul des statistiques\n",
    "    avg_score = np.mean(game_scores)\n",
    "    std_score = np.std(game_scores)\n",
    "    max_score = np.max(game_scores)\n",
    "    \n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    avg_steps = np.mean(steps_survived)\n",
    "    \n",
    "    print(\"\\n=== RÉSULTATS DE L'ÉVALUATION ===\")\n",
    "    print(f\"Score Moyen (Jeu)   : {avg_score:.2f} ± {std_score:.2f}\")\n",
    "    print(f\"Score Max (Jeu)     : {max_score}\")\n",
    "    print(f\"Reward Moyenne (RL) : {avg_reward:.2f}\")\n",
    "    print(f\"Survie Moyenne      : {avg_steps:.1f} steps\")\n",
    "    \n",
    "    return game_scores, total_rewards\n",
    "\n",
    "# --- Exécution de l'évaluation ---\n",
    "# Assurez-vous que 'env' et 'agent' sont bien instanciés (ceux de votre fin d'entraînement)\n",
    "test_scores, test_rewards = evaluate_agent(env, agent, n_episodes=50)\n",
    "\n",
    "# --- Visualisation Graphique ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 1. Histogramme des scores\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(test_scores, bins=10, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution des Scores de Jeu')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Nombre d\\'épisodes')\n",
    "plt.axvline(np.mean(test_scores), color='red', linestyle='dashed', linewidth=1, label=f'Moyenne: {np.mean(test_scores):.1f}')\n",
    "plt.legend()\n",
    "\n",
    "# 2. Boxplot (pour voir la stabilité)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(test_scores)\n",
    "plt.title('Dispersion des Scores (Stabilité)')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks([1], ['Agent DQN'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
